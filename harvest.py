import requests
from bs4 import BeautifulSoup
import psycopg2
from datetime import datetime
from openai import OpenAI
import os
import time
import random

# åˆå§‹åŒ– OpenAI
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

DATABASE_URL = os.environ.get("DATABASE_URL")

def init_db():
    conn = psycopg2.connect(DATABASE_URL)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news (
            id SERIAL PRIMARY KEY,
            title TEXT,
            url TEXT,
            content TEXT,
            source TEXT,
            image_url TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    cur.close()
    conn.close()

def save_news(title, url, content, source, image_url):
    conn = psycopg2.connect(DATABASE_URL)
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO news (title, url, content, source, image_url, created_at)
        VALUES (%s, %s, %s, %s, %s, %s)
    """, (title, url, content, source, image_url, datetime.now()))
    conn.commit()
    cur.close()
    conn.close()

def rewrite_news(title, summary):
    prompt = f"""
    å°†ä»¥ä¸‹æ–°é—»æ”¹å†™æˆåŸåˆ›æ–‡ç« ï¼Œä¸è¦ç›´æ¥å¤åˆ¶åŸæ–‡ï¼Œå¹¶ä¿æŒä¿¡æ¯å‡†ç¡®ï¼š
    æ ‡é¢˜ï¼š{title}
    å†…å®¹ï¼š{summary}
    """
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

def fetch_news():
    print("ğŸŸ¢ å¼€å§‹æŠ“å–æ–°é—»...")
    init_db()
    url = "https://www.sinchew.com.my/"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Referer": "https://www.sinchew.com.my/"
    }

    try:
        resp = requests.get(url, headers=headers, timeout=30)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # æ˜Ÿæ´²ç½‘æ–°é—»åˆ—è¡¨ï¼ˆæ ¹æ®é¦–é¡µç»“æ„é€‰æ‹©åˆé€‚çš„ selectorï¼‰
        articles = soup.select("div.list-group-item a")[:5]  # å‰5æ¡æ–°é—»
        if not articles:
            print("âš ï¸ æ²¡æœ‰æŠ“åˆ°æ–°é—»ï¼Œå¯èƒ½ selector éœ€è¦è°ƒæ•´")
            return

        for a in articles:
            title = a.get_text(strip=True)
            href = a.get("href")
            if not href.startswith("http"):
                href = "https://www.sinchew.com.my" + href

            # æŠ“å–æ­£æ–‡
            try:
                article_resp = requests.get(href, headers=headers, timeout=30)
                article_resp.raise_for_status()
                article_soup = BeautifulSoup(article_resp.text, "html.parser")
                content_div = article_soup.select_one("div.article-content")
                summary = content_div.get_text(strip=True) if content_div else title
            except Exception:
                summary = title

            # æ”¹å†™ä¸ºåŸåˆ›æ–‡ç« 
            rewritten = rewrite_news(title, summary)

            # å›¾ç‰‡
            img_tag = article_soup.find("img")
            image_url = img_tag["src"] if img_tag else ""

            save_news(title, href, rewritten, "SinChew", image_url)
            print(f"âœ… ä¿å­˜æ–°é—»: {title}")

            # éšæœºå»¶æ—¶ 5~10 ç§’ï¼Œé˜²æ­¢è¢«å°
            time.sleep(random.randint(5, 10))

        print("ğŸŸ¢ æŠ“å–å®Œæˆ")

    except requests.exceptions.HTTPError as e:
        print("âŒ HTTP é”™è¯¯:", e)
    except requests.exceptions.RequestException as e:
        print("âŒ ç½‘ç»œè¯·æ±‚é”™è¯¯:", e)
    except Exception as e:
        print("âŒ å…¶ä»–é”™è¯¯:", e)
